{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPrxqkeHvcT8XGKi5wUqPQ+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rismanian1/HW2/blob/main/HW2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0Ssq_ctq6fl"
      },
      "source": [
        "# HW2 - Hamed Rismanian 11249700"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mfxL31yExdJ2",
        "outputId": "01055e2f-d626-43d0-89ae-64bef4d2c9be"
      },
      "source": [
        "!pip install sentencepiece\n",
        "!pip install tensorflow-hub"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/99/e0808cb947ba10f575839c43e8fafc9cc44e4a7a2c8f79c60db48220a577/sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2MB)\n",
            "\r\u001b[K     |▎                               | 10kB 22.6MB/s eta 0:00:01\r\u001b[K     |▌                               | 20kB 28.6MB/s eta 0:00:01\r\u001b[K     |▉                               | 30kB 23.6MB/s eta 0:00:01\r\u001b[K     |█                               | 40kB 21.1MB/s eta 0:00:01\r\u001b[K     |█▍                              | 51kB 22.7MB/s eta 0:00:01\r\u001b[K     |█▋                              | 61kB 16.4MB/s eta 0:00:01\r\u001b[K     |██                              | 71kB 16.7MB/s eta 0:00:01\r\u001b[K     |██▏                             | 81kB 16.0MB/s eta 0:00:01\r\u001b[K     |██▌                             | 92kB 15.4MB/s eta 0:00:01\r\u001b[K     |██▊                             | 102kB 15.7MB/s eta 0:00:01\r\u001b[K     |███                             | 112kB 15.7MB/s eta 0:00:01\r\u001b[K     |███▎                            | 122kB 15.7MB/s eta 0:00:01\r\u001b[K     |███▌                            | 133kB 15.7MB/s eta 0:00:01\r\u001b[K     |███▉                            | 143kB 15.7MB/s eta 0:00:01\r\u001b[K     |████                            | 153kB 15.7MB/s eta 0:00:01\r\u001b[K     |████▍                           | 163kB 15.7MB/s eta 0:00:01\r\u001b[K     |████▋                           | 174kB 15.7MB/s eta 0:00:01\r\u001b[K     |█████                           | 184kB 15.7MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 194kB 15.7MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 204kB 15.7MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 215kB 15.7MB/s eta 0:00:01\r\u001b[K     |██████                          | 225kB 15.7MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 235kB 15.7MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 245kB 15.7MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 256kB 15.7MB/s eta 0:00:01\r\u001b[K     |███████                         | 266kB 15.7MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 276kB 15.7MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 286kB 15.7MB/s eta 0:00:01\r\u001b[K     |████████                        | 296kB 15.7MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 307kB 15.7MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 317kB 15.7MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 327kB 15.7MB/s eta 0:00:01\r\u001b[K     |█████████                       | 337kB 15.7MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 348kB 15.7MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 358kB 15.7MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 368kB 15.7MB/s eta 0:00:01\r\u001b[K     |██████████                      | 378kB 15.7MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 389kB 15.7MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 399kB 15.7MB/s eta 0:00:01\r\u001b[K     |███████████                     | 409kB 15.7MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 419kB 15.7MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 430kB 15.7MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 440kB 15.7MB/s eta 0:00:01\r\u001b[K     |████████████                    | 450kB 15.7MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 460kB 15.7MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 471kB 15.7MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 481kB 15.7MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 491kB 15.7MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 501kB 15.7MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 512kB 15.7MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 522kB 15.7MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 532kB 15.7MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 542kB 15.7MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 552kB 15.7MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 563kB 15.7MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 573kB 15.7MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 583kB 15.7MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 593kB 15.7MB/s eta 0:00:01\r\u001b[K     |████████████████                | 604kB 15.7MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 614kB 15.7MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 624kB 15.7MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 634kB 15.7MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 645kB 15.7MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 655kB 15.7MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 665kB 15.7MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 675kB 15.7MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 686kB 15.7MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 696kB 15.7MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 706kB 15.7MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 716kB 15.7MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 727kB 15.7MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 737kB 15.7MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 747kB 15.7MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 757kB 15.7MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 768kB 15.7MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 778kB 15.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 788kB 15.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 798kB 15.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 808kB 15.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 819kB 15.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 829kB 15.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 839kB 15.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 849kB 15.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 860kB 15.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 870kB 15.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 880kB 15.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 890kB 15.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 901kB 15.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 911kB 15.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 921kB 15.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 931kB 15.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 942kB 15.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 952kB 15.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 962kB 15.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 972kB 15.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 983kB 15.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 993kB 15.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 1.0MB 15.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.0MB 15.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 1.0MB 15.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 1.0MB 15.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 1.0MB 15.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.1MB 15.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 1.1MB 15.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 1.1MB 15.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 1.1MB 15.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 1.1MB 15.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 1.1MB 15.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 1.1MB 15.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.1MB 15.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 1.1MB 15.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 1.1MB 15.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 1.2MB 15.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.2MB 15.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 1.2MB 15.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 1.2MB 15.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 1.2MB 15.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.2MB 15.7MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.95\n",
            "Requirement already satisfied: tensorflow-hub in /usr/local/lib/python3.7/dist-packages (0.11.0)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-hub) (3.12.4)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-hub) (1.19.5)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.8.0->tensorflow-hub) (1.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.8.0->tensorflow-hub) (54.1.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fwxPnIDBq87S",
        "outputId": "3f95449b-0cd1-4cfe-985d-ff871fedcb56"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "print(\"TF version: \", tf.__version__)\n",
        "print(\"Hub version: \", hub.__version__)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TF version:  2.4.1\n",
            "Hub version:  0.11.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k6dSDE3Urvho"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# For cleaning the text\n",
        "import spacy\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import regex as re\n",
        "import string\n",
        "\n",
        "# For visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "\n",
        "# For building our model\n",
        "import tensorflow.keras\n",
        "import sklearn\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.optimizers import SGD, Adam\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.layers import Dense, Input, Dropout, GlobalAveragePooling1D"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VvYny-7yrYw4"
      },
      "source": [
        "### Import the data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tKCvcIkDoxwQ"
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3EmU0LNwPpat"
      },
      "source": [
        "import io\n",
        "\n",
        "TRAIN = pd.read_csv(io.BytesIO(uploaded['train.csv']))"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xBbcC5wfPpim"
      },
      "source": [
        "TEST = pd.read_csv(io.BytesIO(uploaded['test.csv']))"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H1cgzzNaPprx"
      },
      "source": [
        "TEXT = pd.read_csv(io.BytesIO(uploaded['text.csv']))"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SW80Gh75Pp0l"
      },
      "source": [
        "NODEID2PAPERID = pd.read_csv(io.BytesIO(uploaded['nodeid2paperid.csv']))"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fO7_2UAzP3T0"
      },
      "source": [
        "SAMPLE = pd.read_csv(io.BytesIO(uploaded['sample.csv'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KqScJuyjrosK"
      },
      "source": [
        "### Prepare the test set (will be used in final prediction):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aXuIulg8pjP6"
      },
      "source": [
        "test = pd.merge(TEST, NODEID2PAPERID, how='inner', left_on='137832', right_on='node idx') # merging datasets\n",
        "test = pd.merge(test, TEXT, how='inner', left_on='paper id', right_on='630234') # merging datasets\n",
        "test = test.drop(columns=['node idx', 'paper id', '630234']) # Remove unnecessary columns\n",
        "test = test.rename(columns={test.columns[0]: 'id', test.columns[1]: 'title', test.columns[2]: 'abstract'}) # Rename the columns"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wx5FJW9Qr4EU"
      },
      "source": [
        "### Prepare the data (to train models):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HvQwCmRytpcr"
      },
      "source": [
        "data = pd.merge(TRAIN, NODEID2PAPERID, how='inner', left_on='0', right_on='node idx') # merging datasets\n",
        "data = pd.merge(data, TEXT, how='inner', left_on='paper id', right_on='630234') # merging datasets\n",
        "data = data.drop(columns=['0', 'node idx', 'paper id', '630234']) # Remove unnecessary columns\n",
        "data = data.rename(columns={data.columns[0]: 'label', data.columns[1]: 'title', data.columns[2]: 'abstract'}) # Rename the columns"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SkttGwW5ty7-"
      },
      "source": [
        "### Explore the data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6p7oYaYwt1Nk"
      },
      "source": [
        "data.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AEHe-1tZt6Dh"
      },
      "source": [
        "data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r5p7mG4ct8me"
      },
      "source": [
        "# Length of the titles\n",
        "data['title'].apply(len).plot(bins=50, kind='hist')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O8pPu73-t_0d"
      },
      "source": [
        "# Length of the abstracts\n",
        "data['abstract'].apply(len).plot(bins=50, kind='hist')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IBcuAQUPuGEs"
      },
      "source": [
        "# Bar chart of labels\n",
        "sns.countplot(x='label', data=data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KuqhmZjauHLC"
      },
      "source": [
        "### Create a benchmark:\n",
        "\n",
        "From the graphic above, we see that label 16 has the largest size of all the labels. So as a benchmark, we can predict \"16\" as the label of each example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pBqbsQ9NuOvC"
      },
      "source": [
        "# Percentage of each category\n",
        "round(data[\"label\"].value_counts(normalize = True) * 100, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tz3JwIHnuSM-"
      },
      "source": [
        "The accuracy of the benchmark model will be approximately 37%."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LYew1oo9ypsU"
      },
      "source": [
        "### Cleaning the Data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1EJzUk0tx-6J",
        "outputId": "be637e76-0dc2-40a3-fdbc-23b20b109ef7"
      },
      "source": [
        "nlp = spacy.load(\"en\")\n",
        "sp = spacy.load('en_core_web_sm')\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# spacy (362 words)\n",
        "spacy_st = nlp.Defaults.stop_words\n",
        "# nltk(179 words)\n",
        "nltk_st = stopwords.words('english')\n",
        "\n",
        "def clean(txt, http = True, punc = True, lem = True, stop_w = True):\n",
        "    \n",
        "    if http is True:\n",
        "        txt = re.sub(\"https?:\\/\\/t.co\\/[A-Za-z0-9]*\", '', txt)\n",
        "\n",
        "    # stop words\n",
        "    # in here I changed the placement of lower for those of you who want to use\n",
        "    # Cased BERT later on.\n",
        "    if stop_w == 'nltk':\n",
        "        txt = [word for word in word_tokenize(txt) if not word.lower() in nltk_st]\n",
        "        txt = ' '.join(txt)\n",
        "\n",
        "    elif stop_w == 'spacy':\n",
        "        txt = [word for word in word_tokenize(txt) if not word.lower() in spacy_st]\n",
        "        txt = ' '.join(txt)\n",
        "\n",
        "    # lemmitizing\n",
        "    if lem == True:\n",
        "        lemmatized = [word.lemma_ for word in sp(txt)]\n",
        "        txt = ' '.join(lemmatized)\n",
        "\n",
        "    # punctuation removal\n",
        "    if punc is True:\n",
        "        txt = txt.translate(str.maketrans('', '', string.punctuation))\n",
        "        \n",
        "    # removing extra space\n",
        "    txt = re.sub(\"\\s+\", ' ', txt)\n",
        "    \n",
        "    return txt"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nwQan0gF3iO0"
      },
      "source": [
        "data['cleaned_text'] = data.abstract.apply(lambda x: clean(x, lem = True, stop_w = 'nltk', http = True, punc = True))"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MNpdoQnmM0qP"
      },
      "source": [
        "### Word Embeddings:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wuTxoDvgM4HS"
      },
      "source": [
        "# Bert Tokenizer\n",
        "\n",
        "!wget --quiet https://raw.githubusercontent.com/tensorflow/models/master/official/nlp/bert/tokenization.py\n",
        "import tokenization\n",
        "FullTokenizer = tokenization.FullTokenizer"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J3Wnxia8Nb2c",
        "outputId": "3cb2db83-3d0b-41f1-ac93-2bddcd6ed29d"
      },
      "source": [
        "ans = input(\"Which Bert should I use? \\n a. Base uncased \\n b. Large uncased \\n c. Basic cased \\n d. Large cased \\n\")\n",
        "\n",
        "if ans is 'a':\n",
        "    BERT_MODEL_HUB = 'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/2'\n",
        "    disc = 'Base_uncased'\n",
        "elif ans is 'b':\n",
        "    BERT_MODEL_HUB = 'https://tfhub.dev/tensorflow/bert_en_uncased_L-24_H-1024_A-16/2' \n",
        "    disc = 'Large_uncased'\n",
        "elif ans is 'c':\n",
        "    BERT_MODEL_HUB = 'https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/2'\n",
        "    disc = 'Base_cased'\n",
        "elif ans is 'd':\n",
        "    BERT_MODEL_HUB = 'https://tfhub.dev/tensorflow/bert_en_cased_L-24_H-1024_A-16/2'\n",
        "    disc = 'Large_cased'\n",
        "\n",
        "bert_layer = hub.KerasLayer(BERT_MODEL_HUB, trainable=True)\n",
        "print('Bert layer is ready to use!')\n",
        "\n",
        "\n",
        "\n",
        "if ans =='a' or ans =='b':\n",
        "    to_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
        "    vocabulary_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
        "\n",
        "    tokenizer = FullTokenizer(vocabulary_file, to_lower_case)\n",
        "    \n",
        "\n",
        "    \n",
        "elif ans =='c' or ans =='d':\n",
        "    \n",
        "    vocabulary_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
        "    \n",
        "    tokenizer = FullTokenizer(vocabulary_file, do_lower_case=False)\n",
        "\n",
        "\n",
        "print('Bert Tokenizer is ready!!!')"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Which Bert should I use? \n",
            " a. Base uncased \n",
            " b. Large uncased \n",
            " c. Basic cased \n",
            " d. Large cased \n",
            "a\n",
            "Bert layer is ready to use!\n",
            "Bert Tokenizer is ready!!!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sJF179mNO-kP"
      },
      "source": [
        "sentence = 'This is the 2nd assignment of Deep Learning.'\n",
        "print('Tokenized version of {} is : \\n {} '.format(sentence, tokenizer.tokenize(sentence)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yQygIc4-fOev"
      },
      "source": [
        "def tokenize_text(text_):\n",
        "    return tokenizer.convert_tokens_to_ids(['[CLS]'] + tokenizer.tokenize(text_) + ['[SEP]'])\n",
        "\n",
        "data['tokenized_text'] = data.cleaned_text.apply(lambda x: tokenize_text(x))"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KBgI7kZ8gf9k",
        "outputId": "9adffe6f-d5d7-4b20-98de-ac2d30a2d36a"
      },
      "source": [
        "# the maximum length of the tokenized text\n",
        "\n",
        "max_len = len(max(data.tokenized_text, key = len))\n",
        "print('The maximum length is:', max_len)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The maximum length is: 705\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I_eAJz1dhTTF"
      },
      "source": [
        "# padding\n",
        "data['padded_text'] = data.tokenized_text.apply(lambda x: x + [0] * (max_len - len(x)))"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9OB-O9z4iB2n"
      },
      "source": [
        "### Construct the model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wzO3aw02iHiR"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}